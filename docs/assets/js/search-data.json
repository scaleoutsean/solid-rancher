{"0": {
    "doc": "About",
    "title": "About",
    "content": "## About this Site This is a personal site by ScaleoutSean and is not supported, endorsed or funded by any 3rd party (except that it uses free Github resources). This project adheres to [No Code of Conduct](https://nocodeofconduct.com). This site also hosts my personal [blog](https://scaleoutsean.github.io). ## Is this some sort of official advice from Rancher, NetApp, SuSE or some other vendor No. ## License, Trademarks, Ackowledgements The content of solid-rancher repo by ScaleoutSean is licensed under the Do What The F\\*ck You Want To Public License (see [LICENSE](../LICENSE)) except where noted differently. The Solid Rancher repository image was derived from \"Cowboy Hat\" by Charles Rondeau, licensed under Public Domain license. The metallic (\"solid metal\") appearance - perhahps not very successful - is supposed to convey the idea of solid Rancher-based K8s clusters on NetApp HCI or SolidFire clusters. The Web template was designed by Patrick Marsceill and is available under The MIT License. NetApp, ONTAP, SolidFire, SnapMirror and the marks listed at www.netapp.com/TM are trademarks of NetApp, Inc. Rancher Labs, Rancher, Kubernetes, and other brands and marks belong to their respective owners. ",
    "url": "https://scaleoutsean.github.io/solid-rancher/about/",
    "relUrl": "/about/"
  },"1": {
    "doc": "Automate Deployment",
    "title": "Automate Deployment",
    "content": "In addition to deploying Rancher on NetApp HCI or SolidFire, you may also want to automate related resources: - SolidFire storage accounts (for user clusters) - SolidFire volumes (to create Datastores or pre-populated Trident volumes that can be imported to user cluster with NetApp Trident) We should create a dedicated storage account for each User Cluster that needs to use external SolidFire storage through NetApp Trident. ## PowerShell PowerShell is not often used for Kubernetes, but it's quite popular for VMware. If you use PowerCLI to automate vSphere, consider using SolidFire PowerShell Tools to automate SolidFire. ### Example Adjust the sample script from [this repo](https://github.com/scaleoutsean/solidfire-windows). ## Ansible There's over 20 SolidFire modules for Ansible. Virtually all operations can be automated with it. ## Terraform It can be used for more than one thing, but here are some ideas: - Provision VMware storage for Rancher Management or User Clusters - Use SolidFire Provider to create volumes and add them to existing vCenter Volume Access Group - Use vSphere Provider to create Datastores - Use ez-rancher to deploy Rancher - Provision native iSCSI storage for User Clusters - Use SolidFire Provider to create storage account (CHAP user name and password) - Deploy Rancher User Cluster with iSCSI Network interface and configure it use the storage account to configure NetApp Trident SolidFire Provider for Terraform can be downloaded [here](https://registry.terraform.io/providers/NetApp/netapp-elementsw/latest). ### Example Follow the example from [https://github.com/NetApp/terraform-provider-netapp-elementsw](https://github.com/NetApp/terraform-provider-netapp-elementsw/). This example illustrates all resources available so it creates an account, IQN, VAG and volumes. Because with Trident we won't use VAGs, we just need to register a new account (consider using the same name as for Rancher cluster) and optionally (persistent volumes are generally created with Trident) create volumes to import them from Trident. - Management cluster probably doesn't need to use Trident - it only needs a VMware datastore (or better, three new datastores dedicated to Management Cluster VMs) - User cluster likely needs Trident and should use a dedicated storage account ## Need help? Join NetApp Pub Slack (invite link is at the home page) and get help in [`#netapp-hci`](https://netapppub.slack.com/archives/CKJCPVBDY) (account required - if you haven't joined yet, go to the home page and visit the invite link first.) ",
    "url": "https://scaleoutsean.github.io/solid-rancher/automate/automate-deployment.html",
    "relUrl": "/automate/automate-deployment.html"
  },"2": {
    "doc": "Automate and Monitor Operations",
    "title": "Automate and Monitor Operations",
    "content": "Here are some operational and monitoring tips for NetApp HCI and SolidFire ### Operations Popular choices to automate operations in a NetApp HCI, SolidFire or eSDS environment: - HashiCorp Terraform (vSphere, Rancher and SolidFire) - Red Hat Ansible (vSphere, SolidFire, Rancher) - VMware PowerCLI, pyvmomi, vRealize Automation, vRealize Operations - Storage-specific operations: - vCenter storage operations can be done through the Element Plug-in for vCenter (no public API), Ansible, Terraform, SolidFire Tools for PowerShell, SolidFire Python CLI as well as several SDKs (Python, .NET, etc.) - find examples in the Awesome SolidFire repository - NetApp Trident provides the ability to snapshot and import (which also makes it easy to work with clones) SolidFire volumes - Rancher-specific operations: - Please refer to the Rancher documentation for these details (we want to avoid unnecessary duplication or rehashing of that content) #### Updates - NetApp HCI, SolidFire and eSDS are updated from NetApp HCC - NetApp HCI: HCC updates SolidFire firmware and software and firmware of NetApp HCI compute nodes. Vmware software is updated by using standard VMware procedures. Element Plug-in for vCenter (also known as VCP or vCenter Plug-in) must be removed before vCenter upgrade and deployed after, using HCC). See the NetApp IMT site for the information about supported software versions. - NetApp SolidFire: HCC updates SolidFire firmware and software. The note on VCP on NetApp HCI also applies here. - On eSDS, HCC updates SolidFire software. The node on VCP on NetApp HCI also applies here. eSDS runs on Docker so server firmware are updated by the server vendor. - Rancher - Please check NetApp HCC documentation and KB articles - In general, the idea is to not deviate from a Rancher-supported approach unless there are NetApp HCI-specific guidelines - NetApp Trident - Follow the HCC or (if you deploy Trident using Trident documentation) Trident upgrade documentation and compatibility information #### Adding and Removing a User Cluster These are not set in stone but rather a list of possible actions when adding and removing a Rancher User (Workload) Cluster. - Add - Infrastructure: create User Cluster-specific resources (external DNS, certificates, firewall rules, app gateway paths, AD/LDAP resources, etc.) - VMware: create User Cluster-specific network settings - SolidFire: create a new storage account for NetApp Trident on individual User Cluster. Optionally you could also clone (and later import with Trident Import) one or more SolidFire volumes for the new cluster to use (for example, volume with pre-configured Private Docker Registry containing the same 10 ISOs that all other clusters use) - this could be useful to avoid the unnecessary copying of data into the new cluster and - if you keep these consistent, you'll get 10x efficiency on 10 copies in 10 User clusters - Rancher: deploy User Cluster, in User Cluster deploy Trident, create Storage Classes - Infrastructure: configure log forwarding, event and performance, montoring, backup, replication - Remove - Rancher: remove User Cluster from Management Cluster - SolidFire: remove Trident account for User Cluster - VMware: remove User Cluster-specific networks - Infrastructure: undo User Cluster-specific DNS and external configuration (firewall rules, DNS entries, TLS certificates, Active Directory users or groups, etc.) ### Monitoring and Auditing - Officially-recommmended tools and apps: NetApp ActiveIQ (gratis), NetApp Cloud Insights (public cloud-based control plane with on-prem data acquisition; gratis and paid versions are available) - NetApp HCI and SolidFire-related - NetApp Trident gathers basic metrics for SolidFire back-ends and sends them to Kubernetes - [HCI Collector](https://github.com/scaleoutsean/hcicollector) - sfcollector, written in Python, gathers a bunch of SolidFire performance metrics and sends them to Graphite to be visualized with Grafana. HCI Collector can gather server (that is, H-Series compute node) events via SNMP and IPMI - For other tools and log forwarding, including NetApp HCC (Hybrid Cloud Control) please see Awesome SolidFire, there's a ton of stuff in there (Splunk, ELK, Graylog, Nagios...) #### Storage Capacity and Performance Monitoring Environments with multiple user clusters (and likely mutiple storage accounts for Trident) can easily monitor capacity and performance by (Trident) storage account. Alerts can be set to warn when an account us using more than certain amount of capacity or Min IOPS (or other IOPS). HCI Collector, for example, makes it easy to create such dashboards from data gathered by default. You can also use it to plot capacity consumption trends. Of course there is also ActiveIQ and Cloud Insights and more (check Awesome SolidFire). ",
    "url": "https://scaleoutsean.github.io/solid-rancher/automate/automate-operations.html",
    "relUrl": "/automate/automate-operations.html"
  },"3": {
    "doc": "Automate",
    "title": "Automate",
    "content": "## When to consider Automation If you deploy new Management or Workload clusters on a daily or weekly basis, you should probably consider how to use PowerShell, PowerCLI or Terraform to enforce consistency and eliminate human errors in deployment and destruction of such clusters. These techniques are discused in the Awesome SolidFire repo. ## Tools: Ansible, Powershell, Terraform Considering that ez-rancher employs Terraform and there are both SolidFire (\"ElementSW\") and vSphere Providers for Terraform, for this I'd prefer Terraform followed by PowerShell and only then Ansible. But you can choose any approach that works for you. If you have any of these already in place, that would obviously be a factor. You can find examples of SolidFire storage provisioning with Terraform on YouTube. Awesome SolidFire has additional details on SolidFire automation. ## Automation Almost everything can be automated, but a lot of it isn't specific to NetApp HCI or SolidFire. We'll cover only the parts that need our consideration. One-off actions (such as the creation of a Rancher Resource Pool) may be covered at a later time. - Deploy - Create dedicated storage account and deploy NetApp Trident to User Cluster - Create VMware networks, datastores - Different automation for vSphere clusters with VDS vs. VSS - Rancher Management Cluster has different network requirements from User (Workload) Cluster - Update - Modify storage QoS policy (\"retype\") and back-end QoS settings in NetApp Trident - Expand volume size of Trident or VMs - Create additional datastores or networks - Destroy - Undo the creation of resources from Deploy, including Trident storage account if used by a User Cluster The latter two categories are covered in Automation and Monitoring. ",
    "url": "https://scaleoutsean.github.io/solid-rancher/automate/",
    "relUrl": "/automate/"
  },"4": {
    "doc": "Deploy",
    "title": "Deploy",
    "content": " ",
    "url": "https://scaleoutsean.github.io/solid-rancher/deploy",
    "relUrl": "/deploy"
  },"5": {
    "doc": "ez-rancher",
    "title": "ez-rancher",
    "content": "As indicated earlier, unless the official \"Rancher on NetApp HCI\" documentation or NetApp Support tell us differently, Management Clusters should be deployed with HCC. In this section we'll therefore primarily look at other environments such as NetApp SolidFire/eSDS. In the case you are now wondering what's the difference between a NetApp HCI environment and non-HCI environment with SolidFire, in this context a short answer could be \"NetApp HCI is deployed with NDE\", which also means that: - Compute nodes are NetApp HCI H-Series nodes - Compute nodes run vSphere ESXi hypervisor **NOTE**: Best do not enable node auto-replace on nodes that participate in node pools that use Trident. Rancher considers replaceable nodes to be ephermal which means PVs attached to those nodes disappear together with the node to which they're attached. I have't tested how this affects Trident-provisioned PVs (maybe changing Trident delete policy would help preserve such PVs.) ## Deploy Rancher in NetApp SolidFire and eSDS Environments SolidFire and eSDS users should have mNode and HCC available in their environment, but may not be able to deploy Rancher with it. The reason is the compute nodes where Rancher Kubernetes is deployed cannot be managed by HCC, so we cannot use HCC to do this the way it's done on NetApp HCI clusters. ### NetApp Trident Helm Chart for Workload Clusters ez-rancher deploys it by default. You can also add it later. The chart can be found [here](https://github.com/NetApp/ez-rancher-trident-installer). ### Deploy Rancher Management Cluster using ez-rancher ez-rancher doesn't make networking or other choices for you, so good planning is essential. ez-rancher can be downloaded [here](https://github.com/NetApp/ez-rancher). Edit config file according to the instructions and run the tool. It takes 15-20 minutes to complete. YouTube videos of the process (edited to less than 3 minutes each): - Deploy Management Cluster with ez-rancher: [https://www.youtube.com/watch?v=m54PM9dJujE](https://www.youtube.com/watch?v=m54PM9dJujE) - Deploy Workload Cluster with ez-rancher: [https://www.youtube.com/watch?v=2CavtI4Hdh8](https://www.youtube.com/watch?v=2CavtI4Hdh8) ",
    "url": "https://scaleoutsean.github.io/solid-rancher/deploy/ez-rancher.html",
    "relUrl": "/deploy/ez-rancher.html"
  },"6": {
    "doc": "HCC or ez-rancher",
    "title": "HCC or ez-rancher",
    "content": "## Management vs. Workload (User) Clusters We should concern ourselves only with Rancher Management Cluster(s) because Workload Clusters are deployed by Rancher Management Cluster (not HCC.) The deployment of Workload Clusters is likely to be much less prescriptive and drive by common-sense Kubernetes-in-vSphere-VMs considerations. ## Supportability For NetApp HCI administrators HCC is the recommended and (perhaps more importantly) *supported* approach to deploy Rancher Management Cluster on NetApp HCI. For this deployment method, please refer to the NetApp documentation for Rancher on NetApp HCI. ez-rancher can be used instead of HCC, but infrastructure-wise a Management Cluster deployed by ez-rancher should be indistinguishable from an HCC-deployed Management Cluster to ensure its supportability. NetApp HCI users could deploy Management Cluster with HCC and then replicate (not literally - we don't want IP address or other conflicts) with ez-rancher - this should be good enough for Dev/Test Management Clusters, for example. If we document those parameters by glancing at HCC deployment logs, we can then use ez-rancher to deploy Management Cluster in a manner consistent with HCC. Of course NetApp Support would be the judge of that, so the only reliable method to get a supported production Management Clusters is to deploy it with HCC. vSphere administrators in a SolidFire environment have access to HCC, but HCC canonot be used to deploy Rancher. The reason is compute nodes where Rancher Management Cluster is deployed cannot be managed by HCC, so that task would be impossible to accomplish in the way it's done on NetApp HCI compute nodes).SolidFire customers can therefore use ez-rancher or any method supported by Rancher (which includes manual installation of Virtual Machines). ez-rancher will be discussed in next section. ",
    "url": "https://scaleoutsean.github.io/solid-rancher/plan/hcc-vs-ez-rancher.html",
    "relUrl": "/plan/hcc-vs-ez-rancher.html"
  },"7": {
    "doc": "Home",
    "title": "Home",
    "content": "## Solid Rancher: Notes on Rancher Kubernetes with NetApp HCI and SolidFire This site contains my personal notes about deploying Rancher Kubernetes with NetApp HCI, SolidFire and eSDS. Note that Rancher on NetApp HCI only supports HCI out-of-the box. ### What's in here - Plan: things to consider before you Deploy - Deploy: deployment tips and considerations - Protect: protect your services and data (DR, BC, BR) - Automate: work better, faster If you spot a mistake or have a suggestion, [create an issue](https://github.com/scaleoutsean/solid-rancher/issues) and I'll see what I can do about it. ### External Resources Always review the official documentation. Unofficial sites may be useful so they're listed here as well, especially if they're referenced more than once. - Official NetApp HCI Solution Documentation for Rancher on NetApp HCI (not yet available) - [Awesome SolidFire](https://github.com/scaleoutsean/awesome-solidfire) - collection of SolidFire resources - [ez-rancher](https://github.com/NetApp/ez-rancher/) - automated deployment tool for Rancher Management Clusters - [NetApp Trident](https://netapp-trident.readthedocs.io) - NetApp Trident, dynamic storage provisioner for containers - [NetApp Pub - NetApp Community Slack](https://join.slack.com/t/netapppub/shared_invite/zt-jdnzc1o7-Up1AIoNzZ4fEtONPG0_6Fw) - ask your questions in `#netapp-hci` When in doubt, check with NetApp Support. #### Last Build {{ \"now\" | date: site.date_format }} ",
    "url": "https://scaleoutsean.github.io/solid-rancher/",
    "relUrl": "/"
  },"8": {
    "doc": "Other Resources",
    "title": "Other Resources",
    "content": "In addition to deploying Rancher on NetApp HCI or SolidFire, you may also want to automate storage-related resources: - SolidFire storage accounts (for user clusters) - SolidFire volumes (to create Datastores or pre-populated Trident volumes that can be imported to user cluster with NetApp Trident) Please check the Automate section on some ideas around that. ",
    "url": "https://scaleoutsean.github.io/solid-rancher/deploy/infra.html",
    "relUrl": "/deploy/infra.html"
  },"9": {
    "doc": "Network",
    "title": "Network",
    "content": "## Ingress Do you need to expose your services to the Internet? Or only internally? How is your DNS going to look like? Where do you get TLS certificates and how? How will that impact the security of your applications? You should probably get answers to these (but not only) questions before you start planning the rest. Start with the big picture. Rancher is extremely easy to install so that's going to be the least of your problems. None of this is NetApp HCI or SolidFire specific, so you may plan without special considerations. ### External Use a network load balancer (software (Docker container or app on a VM, bare metal server) or hardware appliance) external to your Rancher VMs. [NGINX+](https://www.nginx.com/products/nginx/) is a good choice for enterprise users. There's a free version, too. ### Internal Internal networking is generally easier, but you still need control over internal DNS (over k8s.int.company.com for NLB and \\*.k8s.int.company.com for services, for example) and the ability to get (or issue) certificates signed by local CA. Self-signed certificates (and auto-created CA) are available but that doesn't mean you should use them. [This post](https://www.varonis.com/blog/active-directory-domain-naming-best-practices/) by NetApp's partner Varonis has good tips about internal domain naming. ## Default Networks on NetApp HCI ### With VMware Virtual Standard Switch TODO ### With VMware Virtual Distributed Switch These are created by NDE during deployment and should not be touched: - DVS_HCI_internal_mNode_Network - DVS_HCI_internal_Storage_Network - DVS_HCI_internal_vCenter_Network - DVS_HCI_vMotion - DVS_HCI_ISCSI_A - DVS_HCI_ISCSI_B NetApp HCI has [solution designs](https://docs.netapp.com/us-en/hci-solutions/) for VMware Private Cloud and other Kubenetes distributions which contain examples of network design for Kubernetes on NetApp HCI. ## Rancher Management Cluster This cluster must be able to communicate with at least the first three destinations in the list below: - vCenter of the VMware cluster in which it is installed - NetApp HCI Management Node (HCC services) - All Rancher Workload Clusters it manages (each may have a separate Management VLAN, but they could also share a common Kubernetes Cluster Management VLAN) - NetApp HCI Storage Management IP (i.e. SolidFire MVIP, if NetApp Trident is used for storage provisioning) - Also any other storage Management IP involved (ONTAP SVM Management IP, external NAS, etc.) - It may also need to be able to connect to a backup network, another cluster at the DR site, a gateway to connect to your Public Cloud provider, and so on ## Rancher User Cluster(s) These need to be able to communicate with: - The Rancher Management Cluster that manages it - Storage Network (Worker Nodes only, and only if these use iSCSI or NFS storage provisioned by NetApp Trident) - NFS could be ONTAP Select running on VMware or any ONTAP appliance reachable from Worker NodesA) - iSCSI could be NetApp HCI storage, NetApp E-Series (iSCSI), ONTAP (iSCSI) Optionally these also may need to connect to other networks (for example to another cluster or service in a branch office or public cloud.) ## DNS, NTP and other critical network services As usual: make sure vSphere, HCC/mNode and Rancher requirements are satisfied. If your services can't work without Active Directory or outgoing Internet connectivity, take that into consideration as well. ## Kubernetes Networking Options with Rancher - Flannel - Calico - Canal (Network Isolation option) - default - Weave Network isolation may be preferred for K8s Projects exposed to the Internet. The last three offer the option of modifying the MTU (CNI Plugin MTU Override). If your vSphere workload networks use MTU 9000, you can look into whether using non-default values helps (in terms of preformance, or avoid packet fragmentation, for example). NetApp HCI enforces MTU 9000 on iSCSI networks while other networks can use MTU 1500. Rancher Worker VMs may use MTU 9000 on the networks for iSCSI, but they can also work with iSCSI interfaces set to VM defaults (MTU 1500, for example). Other interfaces may not be able to use jumbo frames if vSphere networking does not support them. RancherOS could set MTU 9000 in cloud-init.yaml like so: ```yaml rancher: docker: extra_args: [--mtu, 9000] ``` ",
    "url": "https://scaleoutsean.github.io/solid-rancher/plan/network.html",
    "relUrl": "/plan/network.html"
  },"10": {
    "doc": "Plan",
    "title": "Plan",
    "content": "Just like vSphere (if not more so), Kubernetes requires fairly detailed planning. If you intend to first run a Dev/Test cluster, you can spend less time on planning and learn as you go. For production-grade clusters, planning should start early - even before you build the VMware cluster and networking. If these foundations are not firm, your Rancher implementation may be less successful. ",
    "url": "https://scaleoutsean.github.io/solid-rancher/plan",
    "relUrl": "/plan"
  },"11": {
    "doc": "Protect Kubernetes Configuration",
    "title": "Protect Kubernetes Configuration",
    "content": "### Rancher snapshots and backups Rancher snapshots are application-level snapshots that do not initiate vSphere or SolidFire snapshots. If you take local snapshots, these will remain in the VMs, so to get to them you either need (one of?) the etcd VMs or need to take and retain period snapshots with vSphere or from SolidFire. Read about them [here](https://rancher.com/docs/rke/latest/en/etcd-snapshots/). #### Backup to S3 In addition to local snapshots, Rancher snapshots can be shipped off to S3-compatible storage such as NetApp StorageGRID or other. ### Rancher VMs Considering that the provisioning can be fully automated, I am not convinced Rancher VMs need to be backed up. It doesn't hurt to set a rotating snapshot schedule for SolidFire datastore hosting VMs used for User Clusters, but recovering to an old snapshot may turn out to be useless (I haven't looked into this yet - I suspect in many falure cases we would need to restore all Master nodes (VMs) at once). If you keep your dynamic K8s data on SolidFire, backup, replicate and snapshot those volumes, but as far as VM that comprise User Clusters are concerned, they should have no unique data because we should be able to reinstate them at will and do it in minutes. ### Container data If containers use PVs on SolidFire, please refer to data protection of SolidFire data. Several backup vendors support NetApp Trident CSI, so they may be able to protect volumes on a Trident-based back-end, as well as local (in-VM) PVs. ### Kubernetes infrastructure data Depending on how and where it's stored, we may need to do something about one or more of the following (in no particular order): - Git - Security certificates (CA, ICA) - Active Directory, MFA/2FA - Logs - Private registry - Network proxy, DNS and switch configuration specific to Rancher ",
    "url": "https://scaleoutsean.github.io/solid-rancher/protect/protect-k8s-config.html",
    "relUrl": "/protect/protect-k8s-config.html"
  },"12": {
    "doc": "Protect Data on SolidFire Storage",
    "title": "Protect Data on SolidFire Storage",
    "content": "Backup integrations . There are backup applications that support Trident and can take a backup of both VMs and container volumes. Take a look at the backup section of Awesome SolidFire. Some - such as Velero - have been reported to work, but haven’t been tested. Kasten by Veeam has been touch-tested and found to work - see the link for additional comments. SolidFire replication . As mentioned in the introduction, SolidFire supports synchronous and asynchronos replication to one or more SolidFire clusters. It also supports SnapMirror Asynchronous which can be useful in the case of LAN-based replication to NetApp FAS or ONTAP Select. It is also possible to use SnapMirror to replicate PVs to cloud-based CVO but bandwidth and security have to be taken into account when planning such deployments. YouTube has several video demos that show how to set up SolidFire replication with PowerShell and how to failover and failback Trident to a replica SolidFire cluster. With the ability of Trident to import iSCSI devices, however, a search-and-replace process (of PV IQNs, from PROD to DR IQNs) used in those demos may no longer be necessary. A comparison to evaluate which may be more suitable when is required. ",
    "url": "https://scaleoutsean.github.io/solid-rancher/protect/protect-solidfire-data.html",
    "relUrl": "/protect/protect-solidfire-data.html"
  },"13": {
    "doc": "Protect",
    "title": "Protect",
    "content": "Disaster Recovery . You can use SolidFire native replication (see Awesome SolidFire) to replicate Datastores and Volumes used by Rancher to a remote site. Both local and remote volumes can take scheduled snapshots. Business Continuity . In essence the storage part of BC is a combination of two procedures: . | Search-and-Replace operation for Trident targets (paths) on PROD vs. DR site | Stop-Reverse-Start (replication) sequence for storage replication | . These aren’t currently documented specifically for the Rancher on HCI solution or even in the NetApp Trident documentation, but use SolidFire, Kuberetes and Trident features and therefore do not directly depend on Rancher itself. In theory we could use vSphere SRM and SolidFire SRA to failover VMs running Rancher, but that would be a complicated approach and I doubt we’ll see it officially recommended. Awesome SolidFire has examples of how Target IQNs differ between clusters. YouTube has a demo of how PowerShell can be used to pair clusters, create replication relationship between two SolidFire storage clusters. It takes 30 seconds. YouTube also has a (longish, 15 min) video with a demonstration of SolidFire storage cluster failover and failback in a K8s/Trident environment. Since Trident started to support Volume Import feature, it is possible - and in fact recommended - to import SolidFire replica volumes, rather than use search-and-replace of volume IQN strings. BC through Kubernetes . If services running on Kubernetes are designed and protected in a storage-independent approach, BC can be obtained independently of SolidFire storage. Example: a Web site backed by a NOSQL cluster with Web and Database containers spread across multiple sites running indepdent NetApp HCI or SolidFire clusters. Backup and Recovery . Rancher provides its own “snapshot” feature that can backup cluster configuration to a local disk or S3 storage such as NetApp StorageGRID or Public Cloud Object Storage. For cluster configuration I would prefer that over VMware or SolidFire snapshots or backups. For cluster data, you could use application backups or a 3rd party backup software that can backup K8s containers (see Awesome SolidFire). Some - such as Kasten - integrate with CSI and also Kuberntes API to provide data and application recovery (see this post to get started). ",
    "url": "https://scaleoutsean.github.io/solid-rancher/protect",
    "relUrl": "/protect"
  },"14": {
    "doc": "Storage",
    "title": "Storage",
    "content": "## Management Cluster Should you create a new datastore or use existing? Choices: - Use existing Datastore - Add new Datastore - Add N new Datastores, for N VMs running Kubernetes etcd/Control Plane services Which one is \"best\"? You guessed it - it depends. In all likelihood, though, a separate VMware Datastore (and therefore a new SolidFire volume) would be a good starting point. N new datastores for N VMs (Nodes) would be better for situations where a higher HA is desirable, as a failure or failover of one Datastore wouldn't impact all etcd nodes at the same time. Multiple Management Clusters are also a possibility, so one could have the important ones deployed across multiple datastores, and less important ones on one dedicated Datastore (or even shared, although Min QoS for such volumes would have to be set with care). The one thing that doesn't sound like a good idea at all is to put Management Cluster VMs on the same Datastore where vCenter or other non-Rancher VMs run. ez-rancher (and presumably HCC) do not assume that Management Cluster would use PVCs. ### Default Datastores on NetApp HCI As of NetApp Deployment Engine 1.8P1, these two get created during HCI deployment: - NetApp-HCI-Datastore-01 - NetApp-HCI-Datastore-02 Personally I'd create one or more new datastores (NetApp-HCI-Datastore-Rancher-0[1,2,3]) for the important Management Cluster. ## Workload Cluster(s) These could be on new Datastore, or spread across two or more, similar to Management Clusters. ez-rancher allows downstream cluster managers to deploy Trident from [catalog](https://github.com/NetApp/ez-rancher/blob/a31e1fb155f0d85c7a9226f3101cb2a782ef0ea7/rancher.tfvars.example#L7), but they need to obtain a dedicated storage account from VI or Storage Administrator. Again, pay attention to Storage QoS settings (especially Min QoS). Personally I would suggest to evaluate (first in non-production) QoS SIOC to see if helps with intra-VM fairness when Rancher workload VMs are sharing one Datastore/Volume. ",
    "url": "https://scaleoutsean.github.io/solid-rancher/plan/storage.html",
    "relUrl": "/plan/storage.html"
  },"15": {
    "doc": "NetApp Trident",
    "title": "NetApp Trident",
    "content": "### HCC Catalog (Helm Chart) vs. \"Classic\" Approach to Trident deployment On NetApp HCI, use whatever the official documentation suggests (likely the Helm Chart (Catalog) approach). SolidFire and eSDS users can use either approach. - Catalog approach - Standardized for Rancher with both NetApp HCI and SolidFire - Good for frequent or repetitive deployments - Not all Trident versions may be available but versions recommended by NetApp HCI for use with Rancher should be available - Well-tested updates with documented update procedures specific to NetApp HCI - Classic approach - You can choose to deploy any Trident version (which can be both good and bad; at the very least you should check which Trident version is supported by NetApp HCC - this information could be found in the Trident documentation or by checking the versions deployed by ez-rancher Trident Catalog). In theory at least any Trident version that supports Kubernetes contained in a Rancher version supported by NetApp HCC should be acceptable. But if you want to be on the safe side, check what versions are supported by NetApp HCC with Rancher and use one of those. - Somewhat manual process that can be automated, but also a source of problems if poorly planned and executed - Uses standard update procedures from NetApp Trident **NOTES**: - If you deploy multiple Rancher clusters against the same NetApp HCI or SolidFire back-end you must ensure that each Rancher cluster (and its Trident instance) use a different storage account so that CHAP authentication does not lead to one Rancher cluster seeing PVs from another - When you destroy a cluster, remember to clean up the account, volumes, IQNs, VLANs, etc if you have no intent to use them again - When assigning storage account names, assuming Rancher cluster have unique names, administrators could base SolidFire storage account names used by Trident upon Rancher cluster names ### NetApp HCC Helm Chart for Workload Clusters NetApp HCC users should be able to deploy it more easily. ez-rancher users can deploy it or add it later. Find it [here](https://github.com/NetApp/ez-rancher-trident-installer). Add this Helm Chart v3 to a User Cluster's Catalog and deploy from the User Cluster's Cluster Explorer (find it under Apps). ### NetApp Trident \"Classic\" This approach uses standard NetApp Trident deployment steps as documented [here](https://netapp-trident.readthedocs.io/). A YouTube video of the process with Rancher v2.5.2 and Trident v20.10 can be viewed [here](https://www.youtube.com/watch?v=pQbt_TzFqw). #### Customize Trident Deployment for Workload Clusters - In Trident back-end configuration file, pick a unique (not `trident`) user name for the Trident account for each individual cluster - On SolidFire array, use the same name or other appropriate convention to \"link\" cluster name with Trident name - For example Rancher cluster `rancher-dc1-az1-mfg-uc01` would use the same account name on SolidFire: ```json { \"version\": 1, \"storageDriverName\": \"solidfire-san\", \"Endpoint\": \"https://admin:nimda@192.168.1.1/json-rpc/11.0\", ... \"TenantName\": \"rancher-dc1-az1-mfg-uc01\", } ``` #### Configure Trident QoS policies with non-overlapping Min-Max ranges As long as [this](https://github.com/NetApp/trident/issues/281) issue is open, it's best to create Trident storage policies with non-overlaping Min-Max ranges unlike in this example: - Slow: 200/2000/5000 - Fast: 500/5000/10000 This should work better: - Slow: 200/1000/5000 - Fast: 2000/5000/10000 ",
    "url": "https://scaleoutsean.github.io/solid-rancher/deploy/trident.html",
    "relUrl": "/deploy/trident.html"
  },"16": {
    "doc": "vCenter and Other Accounts",
    "title": "vCenter and Other Accounts",
    "content": "### vCenter Role to Deploy Rancher Management Cluster(s) For manual (aka DIY) deployments, while we may use a vCenter administrator account, it's better not to. The Rancher documentation has a list of minimum vCenter privileges required for automated deployment of Rancher v2.5 'local' (i.e. Management) cluster. Feel free to work by it or (as of v2.5.2) you can use PowerCLI to load [this text file](../assets/files/rancher-v2.5.2-vcenter-6.7u3.txt) to your vCenter and save some time. If you have a newer version of Rancher or vSphere, take a look at the Rancher documentation to see if anything has changed. With the file you can use PowerCLI to create a Rancher deployment account with these privileges. See this ([example](https://documentation.commvault.com/commvault/v11_sp19/article?p=115101.htm) to get started.) ### SSO and Github Rancher supports SSO and Github integration. Kubernetes cluster managers and users may need to be able to authenticate and get authorized against related endpoints (Active Directory, etc.) ",
    "url": "https://scaleoutsean.github.io/solid-rancher/plan/vcenter.html",
    "relUrl": "/plan/vcenter.html"
  }
}
